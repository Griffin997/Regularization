{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a71c9a0-e2e4-4097-8c7b-00e9ae44ac52",
   "metadata": {},
   "source": [
    "# Code Description\n",
    "This code contains many functions for testing regularization methods across parameter sets and noise realizations\n",
    "\n",
    "Functions use a standard indexing to refer to our 4 parameters: 0 index is c1, 1 is c2, 2 is T21, and 3 is T22\n",
    "Regularization is applied through use of scipy.optimize.curve_fit in the estimate_parameters function\n",
    "\n",
    "Application of NLLS and regularization across parameters and noise realization can be seen below\n",
    "\n",
    "Deep neural network (DNN) regularization models are generated using the Neural Network Regularization file\n",
    "\n",
    "In general, you will be adjusting SNR below and changing the parameter sets under \"Use prior distributions of c1, c2, T21, and T22, then evaluating how effective our different estimation methods are\n",
    "\n",
    "Numerical method to calculate biased CRLB is currently under progress, so work with Dr. Balan to get that sorted\n",
    "\n",
    "This code was primarily written by Ryan Neff (RN) and then furthur developed by Griffin Hampton (GSH)\n",
    "\n",
    "This is the code that is intended to be used during the manuscript that RN is writing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10825879-d5bd-4518-aacc-7cb6bb05b719",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c497587-fadd-4c07-a58d-2a2e7b07ae0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib widget\n",
    "import h5py\n",
    "import scipy\n",
    "import scipy.io\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.optimize import least_squares\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import statistics\n",
    "import math\n",
    "import time\n",
    "import itertools\n",
    "from tqdm import trange\n",
    "from keras.models import load_model\n",
    "from datetime import date\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61de66d5",
   "metadata": {},
   "source": [
    "# Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecc152d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d13eafc1-2088-4b7f-ab2d-c46672cf820c",
   "metadata": {},
   "source": [
    "# Define Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6abc6fd2-c1c1-4a97-88ce-e04fa4850d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SNR = 100\n",
    "n_elements = 128\n",
    "c1 = 0.5\n",
    "c2 = 1 - c1\n",
    "T21 = 50\n",
    "T22 = 100\n",
    "#Weighting term to ensure the c_i and T2_i are roughly the same magnitude\n",
    "ob_weight = 100\n",
    "Nth = 5\n",
    "n_noise_realizations = 500 #500\n",
    "\n",
    "num_multistarts = 10\n",
    "\n",
    "upper_bound = [2,2,500,1500] #Set upper bound on parameters c1, c2, T21, T22, respectively\n",
    "initial = (0.5, 0.5, 250, 750) #Set initial guesses\n",
    "\n",
    "tdata = np.linspace(0, 635, n_elements)\n",
    "lambdas = np.append(0, np.logspace(-7,3,51))\n",
    "\n",
    "explore_corners = False\n",
    "\n",
    "##################### Important for Naming\n",
    "date = date.today()\n",
    "day = date.strftime('%d')\n",
    "month = date.strftime('%B')[0:3]\n",
    "year = date.strftime('%y')\n",
    "\n",
    "##################### Stored Dictionary\n",
    "runInfo = {\n",
    "    \"SNR\": SNR,\n",
    "    'lambdas': lambdas,\n",
    "    'times': tdata,\n",
    "    'noise_realizations': n_noise_realizations\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b1a4dd-d5c4-4dd5-a7ce-eca4abf0c5d4",
   "metadata": {},
   "source": [
    "# Define General Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55e4221",
   "metadata": {},
   "source": [
    "### Small Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd4c518c",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Simple Functions ##############################\n",
    "# Two parameter definition of s(t) with regularization parameter lambda\n",
    "def G(t, con_1, con_2, tau_1, tau_2): \n",
    "    function = con_1*np.exp(-t/tau_1) + con_2*np.exp(-t/tau_2)\n",
    "    return function\n",
    "\n",
    "def G_tilde(lam, SA = 1):\n",
    "    #SA defines the signal amplitude, defaults to 1 for simulated data\n",
    "    def Gt_lam(t, con1, con2, tau1, tau2):\n",
    "        return np.append(G(t, con1, con2, tau1, tau2), [lam*con1/SA, lam*con2/SA, lam*tau1/ob_weight, lam*tau2/ob_weight])\n",
    "    return Gt_lam\n",
    "\n",
    "def G_tilde_linear(T21, T22, lam):\n",
    "    def G_linear(t, c1, c2):\n",
    "        return np.append(G(t, c1, c2, T21, T22), [lam*c1, lam*c2])\n",
    "    return G_linear\n",
    "\n",
    "def add_noise(signal, SNR):\n",
    "    #Given a noiseless signal, adds noise at given SNR and returns a noisy signal\n",
    "    signal_length = len(signal)\n",
    "    noise_sd = signal[0]/SNR\n",
    "    noisy_signal = signal + np.random.normal(0, noise_sd, signal_length)\n",
    "    return noisy_signal\n",
    "\n",
    "def construct_paramList(c1_list, T21_list, T22_list):\n",
    "    preList = [item for item in itertools.product(c1_list, T21_list, T22_list)]\n",
    "    postList = [list(elem) for elem in preList]\n",
    "    [elem.insert(1,1-elem[0]) for elem in postList]\n",
    "    return postList"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd31665",
   "metadata": {},
   "source": [
    "### Parameter Estimation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc70ee46-56a5-40b2-bce8-8babbe4c6b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_parameters(data, lam, n_initials = num_multistarts, post_normalize = True):\n",
    "    #Pick n_initials random initial conditions within the bound, and choose the one giving the lowest model-data mismatch residual\n",
    "    random_residuals = []\n",
    "    estimates = np.zeros((4,n_initials))\n",
    "    for i in range(n_initials):\n",
    "        ic1 = np.random.uniform(0,1)\n",
    "        ic2 = 1-ic1\n",
    "        iT21 = np.random.uniform(0,upper_bound[2])\n",
    "        iT22 = np.random.uniform(0,upper_bound[3])\n",
    "        p0 = [ic1,ic2,iT21,iT22]\n",
    "        \n",
    "        data_tilde = np.append(data, [0,0,0,0])  \n",
    "    \n",
    "        (c1e, c2e, T21e, T22e), cov = curve_fit(G_tilde(lam), tdata, data_tilde, bounds = (0, upper_bound), p0=p0, max_nfev = 4000)\n",
    "        \n",
    "        #Require T22>T21\n",
    "        if T22e > T21e:\n",
    "            estimates[:,i] = [c1e, c2e, T21e, T22e]\n",
    "            estimated_model = G(tdata, c1e, c2e, T21e, T22e)\n",
    "        else:\n",
    "            estimates[:,i] = [c2e, c1e, T22e, T21e]\n",
    "            estimated_model = G(tdata, c2e, c1e, T22e, T21e)\n",
    "        \n",
    "        residual = np.sum((estimated_model - data)**2)\n",
    "        random_residuals.append(residual)\n",
    "    min_residual_idx = np.argmin(random_residuals)\n",
    "    min_residual_estimates = estimates[:,min_residual_idx]\n",
    "\n",
    "    if post_normalize:\n",
    "        ci_sum = min_residual_estimates[0] + min_residual_estimates[1]\n",
    "        min_residual_estimates[0] = min_residual_estimates[0]/ci_sum\n",
    "        min_residual_estimates[1] = min_residual_estimates[1]/ci_sum\n",
    "\n",
    "        \n",
    "    return min_residual_estimates\n",
    "\n",
    "\n",
    "### Unused as of 11/2/22\n",
    "def min_bias_estimates(c1, c2, T21, T22, n=100, lambdas = np.logspace(-7,3, 51), aggregate=True, agg_arr = [1,1,0.01,0.01]):\n",
    "    #Returns aggregate bias, variance, and MSE of the estimates generated \n",
    "    #from using the lambda which minimizes bias for each noise realization\n",
    "    agg_arr = np.array(agg_arr)\n",
    "    noiseless_curve = G(tdata, c1, c2, T21, T22)\n",
    "    \n",
    "    bias = np.zeros(4)\n",
    "    variance = np.zeros(4)\n",
    "    MSE = np.zeros(4)\n",
    "    \n",
    "    min_bias_lambdas = []\n",
    "    min_bias_est = []\n",
    "    for i in range(n):\n",
    "        np.random.seed(i)\n",
    "        data = add_noise(noiseless_curve, SNR)\n",
    "        agg_bias_list = []\n",
    "        temp_estimates = []\n",
    "        for l in range(len(lambdas)):\n",
    "            lam = lambdas[l]\n",
    "            est = estimate_parameters(data, lam)\n",
    "            temp_estimates.append(est)\n",
    "            agg_bias_list.append(np.absolute(est-[c1,c2,T21,T22]).dot(agg_arr)) #L1 error\n",
    "        \n",
    "        min_bias_idx = np.argmin(agg_bias_list)\n",
    "        \n",
    "        min_bias_lambdas.append(lambdas[min_bias_idx])\n",
    "        min_bias_est.append(temp_estimates[min_bias_idx])\n",
    "    \n",
    "    min_bias_est = np.array(min_bias_est)\n",
    "    \n",
    "    c1est = min_bias_est[:,0]\n",
    "    c2est = min_bias_est[:,1]\n",
    "    T21est = min_bias_est[:,2]\n",
    "    T22est = min_bias_est[:,3]\n",
    "    \n",
    "    bias[0] = (c1est - [c1]*n).sum()/n #GSH - check this - why are we multiplying by n - does this do something to the size - what about broadcasting?\n",
    "    bias[1] = (c2est - [c2]*n).sum()/n\n",
    "    bias[2] = (T21est - [T21]*n).sum()/n\n",
    "    bias[3] = (T22est - [T22]*n).sum()/n\n",
    "    \n",
    "    variance[0] = np.var(c1est)\n",
    "    variance[1] = np.var(c2est)\n",
    "    variance[2] = np.var(T21est)\n",
    "    variance[3] = np.var(T22est)\n",
    "    \n",
    "    MSE = bias**2 + variance\n",
    "    \n",
    "    agg_bias = np.absolute(bias).dot(agg_arr)\n",
    "    agg_variance = variance.dot(agg_arr**2)\n",
    "    agg_MSE = MSE.dot(agg_arr**2)\n",
    "    \n",
    "    if aggregate==True:\n",
    "        return agg_bias, agg_variance, agg_MSE, min_bias_lambdas\n",
    "    else:\n",
    "        return bias, variance, MSE, min_bias_lambdas\n",
    "\n",
    "def J(t, con1, con2, tau1, tau2):\n",
    "    func1 = np.exp(-t/tau1)\n",
    "    func2 = np.exp(-t/tau2)\n",
    "    func3 = (con1*t)*np.exp(-t/tau1)/(tau1**2)\n",
    "    func4 = (con2*t)*np.exp(-t/tau2)/(tau2**2)\n",
    "    jacobian = np.stack((func1, func2, func3, func4), axis=-1)\n",
    "    \n",
    "    return jacobian\n",
    "\n",
    "def cov_matrix(con1, con2, tau1, tau2, SNR):\n",
    "    noise_sd = 1/SNR\n",
    "    jacobians = J(tdata, con1, con2, tau1, tau2).transpose()@J(tdata, con1, con2, tau1, tau2) \n",
    "    covariance = np.linalg.inv(jacobians)\n",
    "    return (noise_sd**2)*covariance\n",
    "\n",
    "def get_min_MSE_array(c1_set, c2_set, T21_set, T22_set, lambdas, nReps = n_noise_realizations, verbose=False):\n",
    "    #Given a prior set of true parameters and a parameter of interest, \n",
    "    #find lambdas which give lowest MSE for each combination of parameters\n",
    "    n_c1 = len(c1_set)\n",
    "    n_c2 = len(c2_set)\n",
    "    n_T21 = len(T21_set)\n",
    "    n_T22 = len(T22_set)\n",
    "    n_params = n_c1*n_c2*n_T21*n_T22\n",
    "    n_lambdas = len(lambdas)\n",
    "    \n",
    "    min_MSE_array = np.zeros((n_c1,n_c2,n_T21,n_T22,4))\n",
    "    unreg_MSE_array = np.zeros((n_c1,n_c2,n_T21,n_T22,4))\n",
    "    improvement_array = np.zeros((n_c1,n_c2,n_T21,n_T22,4))\n",
    "    min_lambda_array = np.zeros((n_c1,n_c2,n_T21,n_T22,4))\n",
    "    iterator = 1\n",
    "    start_time = time.time()\n",
    "    for ic1 in range(n_c1):\n",
    "        c1 = c1_set[ic1]\n",
    "        for ic2 in range(n_c2):\n",
    "            c2 = c2_set[ic2]\n",
    "            for iT21 in range(n_T21):\n",
    "                T21 = T21_set[iT21]\n",
    "                for iT22 in range(n_T22):\n",
    "                    T22 = T22_set[iT22]\n",
    "                    if verbose:\n",
    "                        print(f'Calculating combo {iterator} of {n_params}: {np.round(iterator/n_params*100,2)}%: ' +  \n",
    "                                f'Projected time left {(time.time()-start_time)/iterator*(n_params-iterator)/60}min')\n",
    "                        iterator+=1\n",
    "                    p_true = [c1, c2, T21, T22]\n",
    "                    noiseless_curve = G(tdata, c1, c2, T21, T22)\n",
    "                    estimates = np.zeros((nReps,n_lambdas,4))\n",
    "                    for i in trange(nReps):\n",
    "                        for l in range(n_lambdas):\n",
    "                            data = add_noise(noiseless_curve, SNR)\n",
    "                            est = estimate_parameters(data, lam=lambdas[l])\n",
    "                            estimates[i,l,:] = est\n",
    "                    \n",
    "                    bias = (estimates - p_true).sum(axis=0)/nReps\n",
    "                    assert(np.allclose(np.mean(estimates - p_true, axis = 0),bias))\n",
    "                    variance = np.var(estimates, axis=0)\n",
    "                    MSE = variance + bias**2\n",
    "                    \n",
    "                    min_MSE = np.min(MSE, axis=0)\n",
    "                    min_MSE_array[ic1,ic2,iT21,iT22,:] = min_MSE\n",
    "                    \n",
    "                    unreg_MSE = MSE[0,:]\n",
    "                    unreg_MSE_array[ic1,ic2,iT21,iT22,:] = unreg_MSE\n",
    "                    \n",
    "                    improvement = (unreg_MSE - min_MSE)/unreg_MSE\n",
    "                    improvement_array[ic1,ic2,iT21,iT22,:] = improvement\n",
    "                    \n",
    "                    min_MSE_lambda_indx = np.argmin(MSE,axis=0)\n",
    "                    min_MSE_lambdas = lambdas[min_MSE_lambda_indx]\n",
    "                    min_lambda_array[ic1,ic2,iT21,iT22,:] = min_MSE_lambdas\n",
    "                    \n",
    "    return min_MSE_array, min_lambda_array, improvement_array, unreg_MSE_array\n",
    "\n",
    "def get_lam_selection_MSE_array(c1_set, c2_set, T21_set, T22_set, lambdas, lam_select, \n",
    "                                noise_iterations=n_noise_realizations, verbose=False, aggregate=False, safety_factor=2, model = None):\n",
    "    #Lam_select is either 'oracle', 'DP', 'GCV', or 'DNN'\n",
    "    #Defines which method is used to select lambda\n",
    "    n_c1 = len(c1_set)\n",
    "    n_c2 = len(c2_set)\n",
    "    n_T21 = len(T21_set)\n",
    "    n_T22 = len(T22_set)\n",
    "    n_params = n_c1*n_c2*n_T21*n_T22\n",
    "    n_lambdas = len(lambdas)\n",
    "    \n",
    "    MSE_array = np.zeros((n_c1,n_c2,n_T21,n_T22,4))\n",
    "    imp_bias_array = np.zeros((n_c1,n_c2,n_T21,n_T22,4))\n",
    "    imp_var_array = np.zeros((n_c1,n_c2,n_T21,n_T22,4))\n",
    "    unreg_MSE_array = np.zeros((n_c1,n_c2,n_T21,n_T22,4))\n",
    "    improvement_array = np.zeros((n_c1,n_c2,n_T21,n_T22,4))\n",
    "    lambda_array = np.zeros((n_c1,n_c2,n_T21,n_T22,4))\n",
    "    iterator = 1\n",
    "    print(\"Starting lam selection with \" + lam_select)\n",
    "    start_time = time.time()\n",
    "    for ic1 in range(n_c1):\n",
    "        c1 = c1_set[ic1]\n",
    "        for ic2 in range(n_c2):\n",
    "            c2 = c2_set[ic2]\n",
    "            for iT21 in range(n_T21):\n",
    "                T21 = T21_set[iT21]\n",
    "                for iT22 in range(n_T22):\n",
    "                    T22 = T22_set[iT22]\n",
    "                    if verbose:\n",
    "                        print(f'{lam_select}: Calculating combo {iterator} of {n_params}: {np.round(iterator/n_params*100,2)}%: ' +  \n",
    "                                f'Projected time left {(time.time()-start_time)/iterator*(n_params-iterator)/60}min')\n",
    "                        iterator+=1\n",
    "                    p_true = [c1, c2, T21, T22]\n",
    "                    noiseless_curve = G(tdata, c1, c2, T21, T22)\n",
    "                    estimates = np.zeros((noise_iterations,4))\n",
    "                    estimates_unreg = np.zeros((noise_iterations,4))\n",
    "                    for i in trange(noise_iterations):\n",
    "                        data = add_noise(noiseless_curve, SNR)\n",
    "                        if lam_select == 'oracle':\n",
    "                            lam = oracle_lambda(c1, c2, T21, T22, data, lambdas, aggregate=aggregate)[1]\n",
    "                        elif lam_select == 'DP':\n",
    "                            lam = DP_lambda(data, safety_factor, lambdas)\n",
    "                        elif lam_select == 'GCV':\n",
    "                            try:\n",
    "                                lam = GCV_lambda(data, lambdas)\n",
    "                            except:\n",
    "                                lam=0\n",
    "                        elif lam_select == 'DNN':\n",
    "                            lam = DNN_lambda(data, model)\n",
    "                        lambda_array[ic1,ic2,iT21,iT22,:] = lam\n",
    "                        est = estimate_parameters(data, lam)\n",
    "                        est_unreg = estimate_parameters(data, 0)\n",
    "                        estimates[i,:] = est\n",
    "                        estimates_unreg[i,:] = est_unreg\n",
    "                    \n",
    "                    bias = (estimates - p_true).sum(axis=0)/noise_iterations\n",
    "                    variance = np.var(estimates, axis=0)\n",
    "                    MSE = variance + bias**2\n",
    "                    MSE_array[ic1,ic2,iT21,iT22,:] = MSE\n",
    "                    \n",
    "                    unreg_bias = (estimates_unreg - p_true).sum(axis=0)/noise_iterations\n",
    "                    unreg_variance = np.var(estimates_unreg, axis=0)\n",
    "                    unreg_MSE = unreg_variance + unreg_bias**2\n",
    "                    unreg_MSE_array[ic1,ic2,iT21,iT22,:] = unreg_MSE\n",
    "                    \n",
    "                    improvement = (unreg_MSE - MSE)/unreg_MSE\n",
    "                    improvement_array[ic1,ic2,iT21,iT22,:] = improvement\n",
    "\n",
    "                    imp_bias_array[ic1,ic2,iT21,iT22,:] = (unreg_bias - bias)/unreg_bias\n",
    "                    imp_var_array[ic1,ic2,iT21,iT22,:] = (unreg_variance - variance)/unreg_variance\n",
    "                    \n",
    "    return MSE_array, lambda_array, improvement_array, imp_bias_array, imp_var_array\n",
    "\n",
    "def process_parameter_estimates(param):\n",
    "    return 0\n",
    "\n",
    "################## Lambda Selection Methods ########################\n",
    "\n",
    "def DP_lambda(data, safety_factor, disclambdas, fSNR = SNR):\n",
    "    #Returns a value of lambda given the data set, a safety factor, the SD of the noise, and a set of lambdas to iterate over\n",
    "    discrepancy_lambda = 0\n",
    "    noise_sd = data[0]/fSNR\n",
    "    error_norm = len(tdata)*(noise_sd)**2\n",
    "    residual_norm = []\n",
    "    for lam in disclambdas:\n",
    "        est = estimate_parameters(data,lam)\n",
    "\n",
    "        residual_norm.append(((G(tdata, est[0], est[1], est[2], est[3])-data)**2).sum())\n",
    "        \n",
    "        for i in range(len(residual_norm)):\n",
    "            if residual_norm[i] < safety_factor*error_norm:\n",
    "                discrepancy_lambda = disclambdas[i]\n",
    "            else:\n",
    "                break\n",
    "    return discrepancy_lambda\n",
    "\n",
    "def oracle_lambda(c1, c2, T21, T22, data, lambdas, aggregate=False, wgt = np.array([1,1,1/ob_weight,1/ob_weight])):\n",
    "    n_lambdas = len(lambdas)\n",
    "    estimates = np.zeros((n_lambdas,4))\n",
    "    p_true = [c1, c2, T21, T22]\n",
    "    for l in range(n_lambdas):\n",
    "        lam = lambdas[l]\n",
    "        estimates[l,:] = estimate_parameters(data, lam)\n",
    "    error = np.absolute(estimates - p_true)\n",
    "    \n",
    "    if aggregate == True:\n",
    "        #If aggregating, returns the estimates which minimize the weighted sum of the error along with a single lambda\n",
    "        agg_error = np.sum((error*wgt)**2, axis = 1) #error@wgt - L2 vs L1 error\n",
    "        assert(np.size(agg_error) == np.size(lambdas))\n",
    "        min_agg_idx = np.argmin(agg_error)\n",
    "        min_agg_est = estimates[min_agg_idx,:]\n",
    "        min_lambda = lambdas[min_agg_idx]\n",
    "        return min_agg_est, min_lambda\n",
    "    if aggregate == False:\n",
    "        #If not aggregating, returns the estimates for each parameter which minimize their respective errors\n",
    "        #Also returns 4 lambdas, one for each of the 4 parameters\n",
    "        min_idx_array = np.argmin(error,axis=0)\n",
    "        min_est_array = estimates[min_idx_array,:]\n",
    "        min_lambdas_array = lambdas[min_idx_array]\n",
    "        return min_est_array, min_lambdas_array\n",
    "\n",
    "def get_GCV(GCV_data, GCV_lam):\n",
    "    GCVd_tilde = np.append(GCV_data, [0,0,0,0])\n",
    "\n",
    "    (rc1e, rc2e, rT21e, rT22e), rcov = curve_fit(G_tilde(GCV_lam), tdata, GCVd_tilde, bounds = (0, upper_bound), p0=initial, max_nfev = 4000)\n",
    "    \n",
    "    #Require that T22>T21\n",
    "    if rT22e > rT21e:\n",
    "        c1GCV = rc1e\n",
    "        c2GCV = rc2e\n",
    "        T21GCV = rT21e\n",
    "        T22GCV = rT22e\n",
    "    else:\n",
    "        c1GCV = rc2e\n",
    "        c2GCV = rc1e\n",
    "        T21GCV = rT22e\n",
    "        T22GCV = rT21e\n",
    "    wmat = np.array([[1,0,0,0],[0,1,0,0],[0,0,0.01,0],[0,0,0,0.01]])\n",
    "    \n",
    "    GCVjacobian = J(tdata, c1GCV, c2GCV, T21GCV, T22GCV)\n",
    "    GCV_residual = ((G(tdata, c1GCV, c2GCV, T21GCV, T22GCV)-GCV_data)**2).sum()\n",
    "    C_GCV = GCVjacobian@np.linalg.inv(GCVjacobian.transpose()@GCVjacobian+(GCV_lam**2)*wmat.transpose()@wmat)@GCVjacobian.transpose()\n",
    "    (n,n) = C_GCV.shape\n",
    "    identity = np.identity(n)\n",
    "\n",
    "    GCVdenominator = (identity - C_GCV).trace()\n",
    "\n",
    "    GCV = GCV_residual/(GCVdenominator**2)\n",
    "    return GCV\n",
    "\n",
    "def GCV_lambda(GCV_data, GCVlambdas, give_curve=False):\n",
    "    #Given a data set and a set of lambdas to iterate through, gives the lambda which minimizes the GCV equation\n",
    "    GCV_values = []\n",
    "    for GCVlam in GCVlambdas:\n",
    "        GCV_values.append(get_GCV(GCV_data, GCVlam))\n",
    "    min_GCV_lam = GCVlambdas[np.argmin(GCV_values)]\n",
    "    if give_curve:\n",
    "        return min_GCV_lam, GCV_values\n",
    "    else:\n",
    "        return min_GCV_lam\n",
    "\n",
    "def DNN_lambda(data, model):\n",
    "    data_dim = np.reshape(data, (1, len(data)))\n",
    "    DNN_lam = 10**(model(data_dim))\n",
    "    return DNN_lam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a95f3b-4026-4c78-a2bb-a96280fea05f",
   "metadata": {},
   "source": [
    "# Define Weighted Regularization Functions - Generating MSE vs. Lambda Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bf6b855c-6a68-4608-acd5-f8ce76a5aae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_regularization(n_params, param_values, param_distr, lambdas, datasets):\n",
    "    n_c1 = n_params[0]\n",
    "    n_c2 = n_params[1]\n",
    "    n_T21 = n_params[2]\n",
    "    n_T22 = n_params[3]\n",
    "    \n",
    "    c1_values = param_values[0]\n",
    "    c2_values = param_values[1]\n",
    "    T21_values = param_values[2]\n",
    "    T22_values = param_values[3]\n",
    "    \n",
    "    c1_distribution = param_distr[0]\n",
    "    c2_distribution = param_distr[1]\n",
    "    T21_distribution = param_distr[2]\n",
    "    T22_distribution = param_distr[3]\n",
    "    \n",
    "    n_lambdas = len(lambdas)\n",
    "    \n",
    "    weighted_bias = 0\n",
    "    weighted_variance = 0\n",
    "    weighted_MSE = 0\n",
    "    \n",
    "    iterator = 1\n",
    "    total_combos = n_c1 * n_c2 * n_T21 * n_T22\n",
    "    for nc1 in range(n_c1):\n",
    "        for nc2 in range(n_c2):\n",
    "            for nt1 in range(n_T21):\n",
    "                for nt2 in range(n_T22):\n",
    "                    print(f'Calculating combo {iterator} of {total_combos}')\n",
    "                \n",
    "                    con1 = c1_values[nc1]\n",
    "                    con2 = c2_values[nc2]\n",
    "                    tau1 = T21_values[nt1]\n",
    "                    tau2 = T22_values[nt2]\n",
    "                    p_true = [con1, con2, tau1, tau2]\n",
    "                    weight = c1_distribution[nc1]*c2_distribution[nc2]*T21_distribution[nt1]*T22_distribution[nt2]\n",
    "                    \n",
    "                    estimates = np.zeros((n_noise_realizations,n_lambdas,4))\n",
    "                    for i in trange(n_noise_realizations):\n",
    "                        data = datasets[nc1,nc2,nt1,nt2,i,:]\n",
    "                        for l in range(n_lambdas):\n",
    "                            lam = lambdas[l]\n",
    "                            est = estimate_parameters(data, lam)\n",
    "                            estimates[i,l] = est\n",
    "                    \n",
    "                    bias = (estimates - p_true).sum(axis=0)/n_noise_realizations\n",
    "                    variance = np.var(estimates, axis=0)\n",
    "                    MSE = variance + bias**2\n",
    "                    \n",
    "                    weighted_bias += weight*bias\n",
    "                    weighted_variance += weight*variance\n",
    "                    weighted_MSE += weight*MSE\n",
    "    return weighted_bias, weighted_variance, weighted_MSE\n",
    "\n",
    "def weighted_GCV_reg(n_params, param_values, param_distr, lambdas, datasets):\n",
    "    n_c1 = n_params[0]\n",
    "    n_c2 = n_params[1]\n",
    "    n_T21 = n_params[2]\n",
    "    n_T22 = n_params[3]\n",
    "    \n",
    "    c1_values = param_values[0]\n",
    "    c2_values = param_values[1]\n",
    "    T21_values = param_values[2]\n",
    "    T22_values = param_values[3]\n",
    "    \n",
    "    c1_distribution = param_distr[0]\n",
    "    c2_distribution = param_distr[1]\n",
    "    T21_distribution = param_distr[2]\n",
    "    T22_distribution = param_distr[3]\n",
    "    \n",
    "    n_lambdas = len(lambdas)\n",
    "    \n",
    "    weighted_bias = 0\n",
    "    weighted_variance = 0\n",
    "    weighted_MSE = 0\n",
    "    \n",
    "    GCV_lambda_list = []\n",
    "    iterator = 1\n",
    "    total_combos = n_c1 * n_c2 * n_T21 * n_T22\n",
    "    for nc1 in range(n_c1):\n",
    "        for nc2 in range(n_c2):\n",
    "            for nt1 in range(n_T21):\n",
    "                for nt2 in range(n_T22):\n",
    "                    print(f'Calculating combo {iterator} of {total_combos}')\n",
    "                \n",
    "                    con1 = c1_values[nc1]\n",
    "                    con2 = c2_values[nc2]\n",
    "                    tau1 = T21_values[nt1]\n",
    "                    tau2 = T22_values[nt2]\n",
    "                    p_true = [con1, con2, tau1, tau2]\n",
    "                    weight = c1_distribution[nc1]*c2_distribution[nc2]*T21_distribution[nt1]*T22_distribution[nt2]\n",
    "                    \n",
    "                    GCV_estimates = []\n",
    "                    for i in trange(n_noise_realizations):\n",
    "                        data = datasets[nc1,nc2,nt1,nt2,i,:]\n",
    "                        lam = GCV_lambda(data, lambdas)\n",
    "                        GCV_lambda_list.append(lam)\n",
    "    \n",
    "                        GCVEst = estimate_parameters(data, lam)\n",
    "                        GCV_estimates.append(GCVEst)\n",
    "\n",
    "                    GCV_estimates = np.array(GCV_estimates)\n",
    "                    \n",
    "                    GCV_bias = (GCV_estimates - p_true).sum(axis=0)/n_noise_realizations\n",
    "                    GCV_variance = np.var(GCV_estimates, axis=0)\n",
    "                    GCV_MSE = GCV_variance + GCV_bias**2\n",
    "                    \n",
    "                    weighted_bias += weight*GCV_bias\n",
    "                    weighted_variance += weight*GCV_variance\n",
    "                    weighted_MSE += weight*GCV_MSE\n",
    "    return weighted_bias, weighted_variance, weighted_MSE, GCV_lambda_list\n",
    "\n",
    "def weighted_DNN_reg(model, n_params, param_values, param_distr, datasets):\n",
    "    #Tests the DNN over a range of true parameter values, returns the weighted bias, variance, and MSE of the DNN estimates\n",
    "    n_c1 = n_params[0]\n",
    "    n_c2 = n_params[1]\n",
    "    n_T21 = n_params[2]\n",
    "    n_T22 = n_params[3]\n",
    "    \n",
    "    c1_values = param_values[0]\n",
    "    c2_values = param_values[1]\n",
    "    T21_values = param_values[2]\n",
    "    T22_values = param_values[3]\n",
    "    \n",
    "    c1_distribution = param_distr[0]\n",
    "    c2_distribution = param_distr[1]\n",
    "    T21_distribution = param_distr[2]\n",
    "    T22_distribution = param_distr[3]\n",
    "    \n",
    "    weighted_bias = 0\n",
    "    weighted_variance = 0\n",
    "    weighted_MSE = 0\n",
    "    \n",
    "    total_DNN_lambdas = []\n",
    "\n",
    "    iterator = 1\n",
    "    total_combos = n_c1 * n_c2 * n_T21 * n_T22\n",
    "    for nc1 in range(n_c1):\n",
    "        for nc2 in range(n_c2):\n",
    "            for nt1 in range(n_T21):\n",
    "                for nt2 in range(n_T22):\n",
    "                    print(f'Calculating combo {iterator} of {total_combos}')\n",
    "                \n",
    "                    con1 = c1_values[nc1]\n",
    "                    con2 = c2_values[nc2]\n",
    "                    tau1 = T21_values[nt1]\n",
    "                    tau2 = T22_values[nt2]\n",
    "                    p_true = [con1, con2, tau1, tau2]\n",
    "                \n",
    "                    weight = c1_distribution[nc1]*c2_distribution[nc2]*T21_distribution[nt1]*T22_distribution[nt2]\n",
    "                    underlying = G(tdata, con1, con2, tau1, tau2)\n",
    "                    param_datasets = datasets[nc1,nc2,nt1,nt2,:,:]\n",
    "                    DNN_lambdas = np.array(model(param_datasets))\n",
    "            \n",
    "                    DNN_estimates = []\n",
    "                    for i in range(n_noise_realizations):\n",
    "                        data = param_datasets[i,:]\n",
    "                        lam = 10**DNN_lambdas[i]\n",
    "                        total_DNN_lambdas.append(lam)\n",
    "    \n",
    "                        DNNEst = estimate_parameters(data, lam)\n",
    "                        DNN_estimates.append(DNNEst)\n",
    "\n",
    "                    DNN_estimates = np.array(DNN_estimates)\n",
    "                    \n",
    "                    DNN_bias = (DNN_estimates - p_true).sum(axis=0)/n_noise_realizations\n",
    "                    DNN_variance = np.var(DNN_estimates, axis=0)\n",
    "                    DNN_MSE = DNN_variance + DNN_bias**2\n",
    "                    \n",
    "                    weighted_bias += weight*DNN_bias\n",
    "                    weighted_variance += weight*DNN_variance\n",
    "                    weighted_MSE += weight*DNN_MSE\n",
    "    return weighted_bias, weighted_variance, weighted_MSE, total_DNN_lambdas\n",
    "\n",
    "def weighted_oracle_reg(n_params, param_values, param_distr, lambdas, datasets):\n",
    "    n_c1 = n_params[0]\n",
    "    n_c2 = n_params[1]\n",
    "    n_T21 = n_params[2]\n",
    "    n_T22 = n_params[3]\n",
    "    \n",
    "    c1_values = param_values[0]\n",
    "    c2_values = param_values[1]\n",
    "    T21_values = param_values[2]\n",
    "    T22_values = param_values[3]\n",
    "    \n",
    "    c1_distribution = param_distr[0]\n",
    "    c2_distribution = param_distr[1]\n",
    "    T21_distribution = param_distr[2]\n",
    "    T22_distribution = param_distr[3]\n",
    "    \n",
    "    n_lambdas = len(lambdas)\n",
    "    \n",
    "    weighted_bias = 0\n",
    "    weighted_variance = 0\n",
    "    weighted_MSE = 0\n",
    "    \n",
    "    oracle_lambda_list = []\n",
    "    iterator = 1\n",
    "    total_combos = n_c1 * n_c2 * n_T21 * n_T22\n",
    "    for nc1 in range(n_c1):\n",
    "        for nc2 in range(n_c2):\n",
    "            for nt1 in range(n_T21):\n",
    "                for nt2 in range(n_T22):\n",
    "                    print(f'Calculating combo {iterator} of {total_combos}')\n",
    "                \n",
    "                    con1 = c1_values[nc1]\n",
    "                    con2 = c2_values[nc2]\n",
    "                    tau1 = T21_values[nt1]\n",
    "                    tau2 = T22_values[nt2]\n",
    "                    p_true = [con1, con2, tau1, tau2]\n",
    "                    weight = c1_distribution[nc1]*c2_distribution[nc2]*T21_distribution[nt1]*T22_distribution[nt2]\n",
    "                    \n",
    "                    oracle_estimates = []\n",
    "                    for i in trange(n_noise_realizations):\n",
    "                        data = datasets[nc1,nc2,nt1,nt2,i,:]\n",
    "                        oraEst, lam = oracle_lambda(con1, con2, tau1, tau2, data, lambdas, aggregate=True)\n",
    "                        oracle_lambda_list.append(lam)\n",
    "                        oracle_estimates.append(np.array(oraEst))\n",
    "\n",
    "                    oracle_estimates = np.array(oracle_estimates)\n",
    "                    \n",
    "                    ora_bias = (oracle_estimates - p_true).sum(axis=0)/n_noise_realizations\n",
    "                    #Why not just use the mean?\n",
    "                    ora_variance = np.var(oracle_estimates, axis=0)\n",
    "                    ora_MSE = ora_variance + ora_bias**2\n",
    "                    \n",
    "                    weighted_bias += weight*ora_bias\n",
    "                    weighted_variance += weight*ora_variance\n",
    "                    weighted_MSE += weight*ora_MSE\n",
    "    return weighted_bias, weighted_variance, weighted_MSE, oracle_lambda_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed95543-a709-434f-a6ac-7cee1a9c203f",
   "metadata": {},
   "source": [
    "# Generate Data Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "39009d18-b751-4a07-ad81-58bee078711f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:00<00:00, 33429.81it/s]\n"
     ]
    }
   ],
   "source": [
    "datasets = np.zeros((n_noise_realizations, n_elements))\n",
    "noiseless_curve = G(tdata, c1, c2, T21, T22)\n",
    "for n in trange(n_noise_realizations):\n",
    "    np.random.seed(n)\n",
    "    data = add_noise(noiseless_curve, SNR)\n",
    "    datasets[n,:] = data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7920228-1ea3-4ff6-965b-23cffe8af34c",
   "metadata": {},
   "source": [
    "# Use prior distributions of c1, c2, T21, and T22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c3c4660d-59f2-4397-a1db-0281da7815cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 91.15it/s]\n"
     ]
    }
   ],
   "source": [
    "#General idea is to define sets of parameters and test regularization methods across those parameters, \n",
    "#then do a weighted average to see how performance is affected by choice of parameter set\n",
    "#Weighting array to ensure parameters are roughly the same magnitude\n",
    "wgt = np.array([1,1,0.01,0.01])\n",
    "#Number of values in each parameter set, index 0 refers to c1, 1 to c2, 2 to T21, and 3 to T22\n",
    "n_params = [1,1,1,1]\n",
    "n_total_params = n_params[0]*n_params[1]*n_params[2]*n_params[3]\n",
    "\n",
    "n_lambdas = 51 #Number of lambdas to loop through\n",
    "lambdas = np.logspace(-7, 3, n_lambdas) #Set range of lambda values\n",
    "#Defines sets for each parameter using np.linspace\n",
    "c1_values = np.linspace(0.5,0.5, n_params[0])\n",
    "c2_values = np.linspace(0.5,0.5, n_params[1])\n",
    "T21_values = np.linspace(40, 40, n_params[2])\n",
    "T22_values = np.linspace(130, 130, n_params[3])\n",
    "#Parametrized version of above, done to save some space\n",
    "param_values = [c1_values, c2_values, T21_values, T22_values]\n",
    "#Define weighted distributions of the parameters, here just weights each value equally\n",
    "c1_distribution = np.array([1]*n_params[0])\n",
    "c1_distribution = c1_distribution/c1_distribution.sum()\n",
    "c2_distribution = np.array([1]*n_params[1])\n",
    "c2_distribution = c2_distribution/c2_distribution.sum()\n",
    "T21_distribution = np.array([1]*n_params[2])\n",
    "T21_distribution = T21_distribution/T21_distribution.sum()\n",
    "T22_distribution = np.array([1]*n_params[3])\n",
    "T22_distribution = T22_distribution/T22_distribution.sum()\n",
    "#Parametrized distributions to save some space\n",
    "param_distr = [c1_distribution, c2_distribution, T21_distribution, T22_distribution]\n",
    "\n",
    "param_averages = [value@distr for value, distr in zip(param_values, param_distr)]\n",
    "\n",
    "#Generate the data sets and calculate the weighted CRLB over the parameter distribution\n",
    "#Pregenerating the data sets saves some computation time later\n",
    "datasets_distr = np.zeros((n_params[0],n_params[1],n_params[2],n_params[3],n_noise_realizations,n_elements))\n",
    "weighted_CRLB = 0\n",
    "for nc1 in trange(n_params[0]):\n",
    "    for nc2 in range(n_params[1]):\n",
    "        for nt1 in range(n_params[2]):\n",
    "            for nt2 in range(n_params[3]):\n",
    "                \n",
    "                con1 = c1_values[nc1]\n",
    "                #con2 = c2_values[nc2]\n",
    "                con2 = 1-con1\n",
    "                tau1 = T21_values[nt1]\n",
    "                tau2 = T22_values[nt2]\n",
    "                \n",
    "                cov = cov_matrix(con1, con2, tau1, tau2, SNR)\n",
    "                weighted_CRLB += np.array([cov[0,0],cov[1,1],cov[2,2],cov[3,3]])/n_total_params  \n",
    "                p_true = [con1, con2, tau1, tau2]\n",
    "                \n",
    "                underlying = G(tdata, con1, con2, tau1, tau2)\n",
    "                datasets = []\n",
    "                for i in range(n_noise_realizations):\n",
    "                    np.random.seed(i)\n",
    "                    data = add_noise(noiseless_curve, SNR)\n",
    "                    datasets_distr[nc1,nc2,nt1,nt2,i,:] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8abd6564-f44b-4bd7-a6dd-8c51ac60314a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if figure1_opt:\n",
    "    #Test the different methods over the parameter distributions\n",
    "    start_time1 = time.time()\n",
    "    weighted_min_bias, weighted_min_variance, weighted_min_MSE, min_lambdas = weighted_oracle_reg(\n",
    "        n_params, param_values, param_distr, lambdas, datasets_distr)\n",
    "    print('Oracle runtime:', time.time() - start_time1, 'seconds')\n",
    "\n",
    "    start_time2 = time.time()\n",
    "    bias_weighted, variance_weighted, MSE_weighted = weighted_regularization(\n",
    "        n_params, param_values, param_distr, lambdas, datasets_distr)\n",
    "    print('NLLS runtime:', time.time() - start_time2, 'seconds')\n",
    "\n",
    "    start_time3 = time.time()\n",
    "    weighted_GCV_bias, weighted_GCV_variance, weighted_GCV_MSE, GCV_lambdas = weighted_GCV_reg(\n",
    "        n_params, param_values, param_distr, lambdas, datasets_distr)\n",
    "    print('GCV runtime:', time.time() - start_time3, 'seconds')\n",
    "    start_time4 = time.time()\n",
    "    # model = load_model('DNN Networks//reg_model SNR 100 3-23.h5')\n",
    "    # weighted_DNN_bias, weighted_DNN_variance, weighted_DNN_MSE, DNN_lambdas = weighted_DNN_reg(\n",
    "        # model, n_params, param_values, param_distr,datasets_distr)\n",
    "    # print('DNN runtime:', time.time() - start_time4, 'seconds')\n",
    "\n",
    "    print('SNR:', SNR)\n",
    "    print('Weighted CRLB:', weighted_CRLB)\n",
    "    print('Weighted uMSE:', MSE_weighted[0,:])\n",
    "    print('Weighted GCV MSE:', weighted_GCV_MSE)\n",
    "    # print('Weighted DNN MSE:', weighted_DNN_MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7f7ee914-f610-4c26-bda6-7b5525cd3f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "if figure1_opt:\n",
    "    fig, ax = plt.subplots(1, 2, figsize = (15,6), tight_layout=True)\n",
    "    fig.suptitle('Lambda selection histograms, SNR = %s'%SNR)\n",
    "    ax[0].hist(np.log10(min_lambdas), bins=20)\n",
    "    ax[1].hist(np.log10(GCV_lambdas), bins=20)\n",
    "    # ax[2].hist(np.log10(DNN_lambdas), bins=20)\n",
    "\n",
    "    ax[0].set_xlabel('log10(lambda)')\n",
    "    ax[0].set_ylabel('Counts')\n",
    "    ax[0].set_title('Optimal, minimum error lambdas')\n",
    "    ax[1].set_xlabel('log10(lambda)')\n",
    "    ax[1].set_ylabel('Counts')\n",
    "    ax[1].set_title('GCV lambdas')\n",
    "    # ax[2].set_xlabel('log10(lambda)')\n",
    "    # ax[2].set_ylabel('Counts')\n",
    "    # ax[2].set_title('Neural network lambdas')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b365bd50-2dbb-4274-a614-8b54eeb44338",
   "metadata": {},
   "outputs": [],
   "source": [
    "if figure1_opt:    \n",
    "    figure, axis = plt.subplots(2, 2, figsize=(12, 8),tight_layout=True)\n",
    "\n",
    "    figure.suptitle('Regularized Mean Square Errors vs. Conventional NLLS, SNR = %s'%SNR)\n",
    "\n",
    "    axis[0,0].set_title('c1 MSE vs. Lambda')\n",
    "    axis[0,0].loglog(lambdas, MSE_weighted[:,0], label = 'c1 MSE', color='b')\n",
    "    axis[0,0].axhline(y=MSE_weighted[0,0], label = 'c1 Unreg MSE', color='orange')\n",
    "    #axis[0,0].axhline(y=weighted_CRLB[0], label = 'c1 CRLB',color='c')\n",
    "    axis[0,0].axhline(y=weighted_GCV_MSE[0], label = 'c1 GCV MSE',color='m')\n",
    "    # axis[0,0].axhline(y=weighted_DNN_MSE[0], label = 'c1 DNN MSE', color='r')\n",
    "    axis[0,0].axhline(y=weighted_min_MSE[0], label = 'c1 oracle MSE', color='g')\n",
    "    axis[0,0].set_ylabel('Mean Square Error')\n",
    "    axis[0,0].set_xlabel('Lambda')\n",
    "    axis[0,0].legend()\n",
    "\n",
    "    axis[0,1].set_title('c2 MSE vs. Lambda')\n",
    "    axis[0,1].loglog(lambdas, MSE_weighted[:,1], label = 'c2 MSE', color='b')\n",
    "    axis[0,1].axhline(y=MSE_weighted[0,1], label = 'c2 Unreg MSE', color='orange')\n",
    "    #axis[0,1].axhline(y=weighted_CRLB[1], label = 'c2 CRLB',color='c')\n",
    "    axis[0,1].axhline(y=weighted_GCV_MSE[1], label = 'c2 GCV MSE',color='m')\n",
    "    # axis[0,1].axhline(y=weighted_DNN_MSE[1], label = 'c2 DNN MSE', color='r')\n",
    "    axis[0,1].axhline(y=weighted_min_MSE[1], label = 'c2 oracle MSE', color='g')\n",
    "    axis[0,1].set_ylabel('Mean Square Error')\n",
    "    axis[0,1].set_xlabel('Lambda')\n",
    "    axis[0,1].legend()\n",
    "\n",
    "    axis[1,0].set_title('T21 MSE vs. Lambda')\n",
    "    axis[1,0].loglog(lambdas, MSE_weighted[:,2], label = 'T21 MSE', color='b')\n",
    "    axis[1,0].axhline(y=MSE_weighted[0,2], label = 'T21 Unreg MSE', color='orange')\n",
    "    #axis[1,0].axhline(y=weighted_CRLB[2], label = 'T21 CRLB',color='c')\n",
    "    axis[1,0].axhline(y=weighted_GCV_MSE[2], label = 'T21 GCV MSE',color='m')\n",
    "    # axis[1,0].axhline(y=weighted_DNN_MSE[2], label = 'T21 DNN MSE', color='r')\n",
    "    axis[1,0].axhline(y=weighted_min_MSE[2], label = 'T21 oracle MSE', color='g')\n",
    "    axis[1,0].set_ylabel('Mean Square Error')\n",
    "    axis[1,0].set_xlabel('Lambda')\n",
    "    axis[1,0].legend()\n",
    "\n",
    "    axis[1,1].set_title('T22 MSE vs. Lambda')\n",
    "    axis[1,1].loglog(lambdas, MSE_weighted[:,3], label = 'T22 MSE', color='b')\n",
    "    axis[1,1].axhline(y=MSE_weighted[0,3], label = 'T22 Unreg MSE', color='orange')\n",
    "    #axis[1,1].axhline(y=weighted_CRLB[3], label = 'T22 CRLB',color='c')\n",
    "    axis[1,1].axhline(y=weighted_GCV_MSE[3], label = 'T22 GCV MSE',color='m')\n",
    "    # axis[1,1].axhline(y=weighted_DNN_MSE[3], label = 'T22 DNN MSE', color='r')\n",
    "    axis[1,1].axhline(y=weighted_min_MSE[3], label = 'T22 oracle MSE', color='g')\n",
    "    axis[1,1].set_ylabel('Mean Square Error')\n",
    "    axis[1,1].set_xlabel('Lambda')\n",
    "    axis[1,1].legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdf23457-c848-4bdf-94e9-afc83962cbe1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'figure1_opt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mif\u001b[39;00m figure1_opt:\n\u001b[0;32m      2\u001b[0m     figure, axis \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39msubplots(\u001b[39m3\u001b[39m, \u001b[39m4\u001b[39m, figsize\u001b[39m=\u001b[39m(\u001b[39m30\u001b[39m, \u001b[39m12.8\u001b[39m),tight_layout\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m     plabels \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mc1\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mc2\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mT21\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mT22\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'figure1_opt' is not defined"
     ]
    }
   ],
   "source": [
    "if figure1_opt:\n",
    "    figure, axis = plt.subplots(3, 4, figsize=(30, 12.8),tight_layout=True)\n",
    "\n",
    "    plabels = ['c1','c2','T21','T22']\n",
    "\n",
    "    figure.suptitle('Weighted Bias, Variance, and MSE vs Lambda, SNR = %s' % SNR)\n",
    "    for p in range(4):\n",
    "        axis[0,p].semilogx(lambdas, bias_weighted[:,p], label='%s weighted bias'%plabels[p], color='b')\n",
    "        axis[0,p].axhline(y=weighted_GCV_bias[p], label = '%s GCV Bias'%plabels[p], color='m')\n",
    "        # axis[0,p].axhline(y=weighted_DNN_bias[p], label = '%s DNN Bias'%plabels[p], color='r')\n",
    "        axis[0,p].set_ylabel('%s weighted bias'%plabels[p])\n",
    "        \n",
    "        axis[1,p].semilogx(lambdas, variance_weighted[:,p], label='%s weighted variance'%plabels[p], color='b')\n",
    "        axis[1,p].axhline(y=weighted_GCV_variance[p], label = '%s GCV Variance'%plabels[p], color='m')\n",
    "        # axis[1,p].axhline(y=weighted_DNN_variance[p], label = '%s DNN Variance'%plabels[p], color='r')\n",
    "        axis[1,p].set_ylabel('%s weighted variance'%plabels[p])\n",
    "        \n",
    "        axis[2,p].loglog(lambdas, MSE_weighted[:,p], label='%s weighted MSE'%plabels[p], color='b')\n",
    "        #axis[2,p].axhline(y=param_CRLB[p], label = '%s weighted CRLB'%plabels[p])\n",
    "        axis[2,p].axhline(y=weighted_GCV_MSE[p], label='%s weighted GCV MSE'%plabels[p], color='m')\n",
    "        # axis[2,p].axhline(y=weighted_DNN_MSE[p], label='%s weighted DNN MSE'%plabels[p], color='r')\n",
    "        axis[2,p].set_ylabel('%s weighted CoV'%plabels[p])\n",
    "    for i in range(3):\n",
    "        for j in range(4):\n",
    "            axis[i,j].legend()\n",
    "            axis[i,j].set_xlabel('Lambda')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eea75f5-61b6-4d30-a742-400f64ec82e4",
   "metadata": {},
   "source": [
    "### Show an aggregate measure of bias, variance, and MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "88d06cee-3322-4738-8a3e-abe1a59314a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if figure1_opt:\n",
    "    aggregate_bias = np.absolute(bias_weighted).dot(wgt)\n",
    "    aggregate_variance = np.array(variance_weighted).dot(wgt**2)\n",
    "    aggregate_MSE = np.array(MSE_weighted).dot(wgt**2)\n",
    "    aggregate_CoV = (np.sqrt(MSE_weighted)/param_averages).sum(axis=-1)/4\n",
    "\n",
    "    agg_GCV_bias = np.absolute(weighted_GCV_bias).dot(wgt)\n",
    "    agg_GCV_variance = weighted_GCV_variance.dot(wgt**2)\n",
    "    agg_GCV_MSE = weighted_GCV_MSE.dot(wgt**2)\n",
    "    agg_GCV_CoV = (np.sqrt(weighted_GCV_MSE)/param_averages).sum()/4\n",
    "\n",
    "    # agg_DNN_bias = np.absolute(weighted_DNN_bias).dot(wgt)\n",
    "    # agg_DNN_variance = np.array(weighted_DNN_variance).dot(wgt**2)\n",
    "    # agg_DNN_MSE = np.array(weighted_DNN_MSE).dot(wgt**2)\n",
    "    # agg_DNN_CoV = (np.sqrt(weighted_DNN_MSE)/param_averages).sum()/4\n",
    "\n",
    "    agg_min_bias = np.absolute(weighted_min_bias).dot(wgt)\n",
    "    agg_min_variance = np.array(weighted_min_variance).dot(wgt**2)\n",
    "    agg_min_MSE = np.array(weighted_min_MSE).dot(wgt**2)\n",
    "    agg_min_CoV = (np.sqrt(weighted_min_MSE)/param_averages).sum()/4\n",
    "\n",
    "    #agg_DNN_pdf_bias = np.absolute(DNN_pdf_bias).dot(wgt)\n",
    "    #agg_DNN_pdf_variance = np.array(DNN_pdf_variance).dot(wgt**2)\n",
    "    #agg_DNN_pdf_MSE = np.array(DNN_pdf_MSE).dot(wgt**2)\n",
    "    #agg_DNN_pdf_CoV = (np.sqrt(DNN_pdf_MSE)/param_averages).dot(wgt)\n",
    "\n",
    "    #agg_CRLB = np.array(param_CRLB).dot(wgt**2)\n",
    "    #agg_CRLB_adjusted = (np.sqrt(param_CRLB)/param_averages).dot(wgt)\n",
    "\n",
    "    fig, ax = plt.subplots(3,1,figsize=(8,12), tight_layout=True)\n",
    "    fig.suptitle('Aggregate measures vs Lambda, SNR = %s' % SNR)\n",
    "    ax[0].semilogx(lambdas, aggregate_bias, label='Aggregate Bias', color='b')\n",
    "    ax[0].semilogx(lambdas, [agg_GCV_bias]*n_lambdas, label = 'Aggregate GCV Bias',color='m')\n",
    "    # ax[0].semilogx(lambdas, [agg_DNN_bias]*n_lambdas, label = 'Aggregate DNN Bias', color='r')\n",
    "    #ax[0].semilogx(lambdas, [agg_DNN_pdf_bias]*n_lambdas, label = 'Aggregate DNN PDF Bias')\n",
    "    ax[0].semilogx(lambdas, [agg_min_bias]*n_lambdas, label = 'Min tot error', color='g')\n",
    "    ax[0].set_ylabel('Bias')\n",
    "    ax[0].set_xlabel('Lambda')\n",
    "    ax[0].set_title('Aggregate Measure of Bias')\n",
    "    ax[0].legend()\n",
    "\n",
    "    ax[1].semilogx(lambdas, aggregate_variance, label='Aggregate Variance', color='b')\n",
    "    ax[1].semilogx(lambdas, [agg_GCV_variance]*n_lambdas, label = 'Aggregate GCV Variance',color='m')\n",
    "    # ax[1].semilogx(lambdas, [agg_DNN_variance]*n_lambdas, label = 'Aggregate DNN Variance', color='r')\n",
    "    #ax[1].semilogx(lambdas, [agg_DNN_pdf_variance]*n_lambdas, label = 'Aggregate DNN PDF Variance')\n",
    "    ax[1].semilogx(lambdas, [agg_min_variance]*n_lambdas, label = 'Min tot error', color='g')\n",
    "    ax[1].set_ylabel('Variance')\n",
    "    ax[1].set_xlabel('Lambda')\n",
    "    ax[1].set_title('Aggregate Measure of Variance')\n",
    "    ax[1].legend()\n",
    "\n",
    "    ax[2].loglog(lambdas, aggregate_MSE, label='Aggregate MSE', color='b')\n",
    "    ax[2].loglog(lambdas, [agg_GCV_MSE]*n_lambdas, label = 'Aggregate GCV MSE',color='m')\n",
    "    # ax[2].loglog(lambdas, [agg_DNN_MSE]*n_lambdas, label = 'Aggregate DNN MSE', color='r')\n",
    "    #ax[2].loglog(lambdas, [agg_DNN_pdf_MSE]*n_lambdas, label = 'Aggregate DNN PDF CoV')\n",
    "    #ax[2].loglog(lambdas, [agg_CRLB_adjusted]*n_lambdas, label = 'Aggregate adjusted CRLB')\n",
    "    ax[2].loglog(lambdas, [agg_min_MSE]*n_lambdas, label = 'Min tot error', color='g')\n",
    "    ax[2].set_ylabel('MSE')\n",
    "    ax[2].set_xlabel('Lambda')\n",
    "    ax[2].set_title('Aggregate Measure of MSE')\n",
    "    ax[2].legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fd008c-a0b6-4f28-9dae-ebd5e314bfe6",
   "metadata": {},
   "source": [
    "# Use a numerical method to calculate the biased CRLB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1041d7-fb9f-4e2e-9cf8-5db8ba29c9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_P_theta_n(theta_n, c_range, T2_range, M):\n",
    "    #theta_n are the 4 parameters to perturb around, given as a numpy array\n",
    "    #c_range and T2_range are define the range of the perturbations about ci and T2i\n",
    "    #M is the number of total perturbations\n",
    "    #The first entry in P_theta_n is always just theta_n\n",
    "    P_theta_n = np.zeros((4, M+1))\n",
    "    P_theta_n[:,0] = theta_n\n",
    "    for m in range(M):\n",
    "        a1_m = np.random.uniform(low=c_range[0],high=c_range[1])\n",
    "        a2_m = np.random.uniform(low=c_range[0],high=c_range[1])\n",
    "        tau1_m = np.random.uniform(low=T2_range[0],high=T2_range[1])\n",
    "        tau2_m = np.random.uniform(low=T2_range[0],high=T2_range[1])\n",
    "        \n",
    "        T_theta_nm = np.append(a1_m, [a2_m, tau1_m, tau2_m])\n",
    "        \n",
    "        P_theta_n[:, m+1] = theta_n + T_theta_nm\n",
    "        \n",
    "    return P_theta_n\n",
    "\n",
    "def construct_A_n(P_theta_n):\n",
    "    #Gives matrix A_n for least squares minimization to get biased CRLB for parameters theta_n\n",
    "    #P_theta_n is the perturbation matrix about some theta_n\n",
    "    M = P_theta_n.shape[1]-1\n",
    "    theta_n = P_theta_n[:,0]\n",
    "    \n",
    "    #Get first four rows of A_n first\n",
    "    z = P_theta_n[:,1] - theta_n\n",
    "    B1 = [z[0],0,0,0,z[1],0,0,0,z[2],0,0,0,z[3],0,0,0]\n",
    "    B2 = [0,z[0],0,0,0,z[1],0,0,0,z[2],0,0,0,z[3],0,0]\n",
    "    B3 = [0,0,z[0],0,0,0,z[1],0,0,0,z[2],0,0,0,z[3],0]\n",
    "    B4 = [0,0,0,z[0],0,0,0,z[1],0,0,0,z[2],0,0,0,z[3]]\n",
    "    A_n = np.stack((B1, B2, B3, B4),axis=0)\n",
    "    #Then get the rest of the rows\n",
    "    for m in range(M-1):\n",
    "        z = P_theta_n[:,m+2] - theta_n\n",
    "        B1 = [z[0],0,0,0,z[1],0,0,0,z[2],0,0,0,z[3],0,0,0]\n",
    "        B2 = [0,z[0],0,0,0,z[1],0,0,0,z[2],0,0,0,z[3],0,0]\n",
    "        B3 = [0,0,z[0],0,0,0,z[1],0,0,0,z[2],0,0,0,z[3],0]\n",
    "        B4 = [0,0,0,z[0],0,0,0,z[1],0,0,0,z[2],0,0,0,z[3]]\n",
    "        \n",
    "        B_mn = np.stack((B1, B2, B3, B4), axis=0)\n",
    "        A_n = np.concatenate((A_n, B_mn),axis=0)\n",
    "        \n",
    "    return A_n\n",
    "\n",
    "def run_NR_P_theta_n(P_theta_n, n_noise_realizations):\n",
    "    M = P_theta_n.shape[1]\n",
    "    estimates = np.zeros((4, M, n_noise_realizations))\n",
    "    for m in range(M):\n",
    "        [c1, c2, T21, T22] = P_theta_n[:,m]\n",
    "        \n",
    "        #np.random.seed(m)\n",
    "        noiseless_curve = G(tdata, c1, c2, T21, T22)\n",
    "        for i in range(n_noise_realizations):\n",
    "            noise_realization = add_noise(noiseless_curve, SNR)\n",
    "            est = estimate_parameters(noise_realization, 0)\n",
    "            estimates[:,m,i] = est\n",
    "    return estimates\n",
    "\n",
    "def x_cost_fun(bias_var_n, A_n):\n",
    "    def function(x):\n",
    "        #For giving least squares the vector of the residual\n",
    "        return (bias_var_n - A_n@x)\n",
    "    return function\n",
    "\n",
    "def biased_CRLB(theta_n, c_range, T2_range, M, n_noise_realizations):\n",
    "    #Calculates the biased CRLB about a set of parameters theta_n\n",
    "    P_theta_n = construct_P_theta_n(theta_n, c_range, T2_range, M)\n",
    "    estimates = run_NR_P_theta_n(P_theta_n, n_noise_realizations)\n",
    "    \n",
    "    sample_mean = np.mean(estimates,axis=-1)\n",
    "    Q_0 = sample_mean[:,0]\n",
    "    sample_mean_shortened = sample_mean[:,1:]\n",
    "    \n",
    "    bias_var_n = np.reshape((sample_mean_shortened.transpose() - Q_0),-1)\n",
    "    \n",
    "    A_n = construct_A_n(P_theta_n)\n",
    "    \n",
    "    minimization = x_cost_fun(bias_var_n, A_n)\n",
    "    x_hat_n = least_squares(minimization, np.zeros(16))['x']\n",
    "    \n",
    "    #x_hat_n = (np.linalg.inv(A_n.transpose()@A_n)@A_n.transpose())@bias_var_n\n",
    "    S_hat_n = np.transpose(np.reshape(x_hat_n, (4,4)))\n",
    "    CRLB_n = cov_matrix(theta_n[0],theta_n[1],theta_n[2],theta_n[3], SNR)\n",
    "\n",
    "    biased_CRLB = S_hat_n@CRLB_n@S_hat_n.transpose()\n",
    "    \n",
    "    return biased_CRLB, S_hat_n\n",
    "\n",
    "def check_CRLB_convergence(n_set, theta_n, c_range, T2_range, M, verbose=False):\n",
    "    n_checks = len(n_set)\n",
    "    P_theta_n = construct_P_theta_n(theta_n, c_range, T2_range, M)\n",
    "    CRLB_n = cov_matrix(theta_n[0],theta_n[1],theta_n[2],theta_n[3], SNR)\n",
    "    biased_CRLB_set = []\n",
    "    c1_CRLB_set = []\n",
    "    c2_CRLB_set = []\n",
    "    T21_CRLB_set = []\n",
    "    T22_CRLB_set = []\n",
    "    sensitivity_set = []\n",
    "    for i in trange(n_checks):\n",
    "        if i == 0:\n",
    "            n_noise_realizations = n_set[i]\n",
    "            estimates = run_NR_P_theta_n(P_theta_n, n_noise_realizations)\n",
    "        else:\n",
    "            n_noise_realizations = n_set[i] - n_set[i-1]\n",
    "            new_estimates = run_NR_P_theta_n(P_theta_n, n_noise_realizations)\n",
    "            estimates = np.append(estimates, new_estimates,axis=-1)\n",
    "        sample_mean = np.mean(estimates, axis=-1)\n",
    "        Q_0 = sample_mean[:,0]\n",
    "        sample_mean_shortened = sample_mean[:,1:]\n",
    "        bias_var_n = np.reshape((sample_mean_shortened.transpose() - Q_0),-1)\n",
    "        A_n = construct_A_n(P_theta_n)\n",
    "        \n",
    "        minimization = x_cost_fun(bias_var_n, A_n)\n",
    "        x_hat_n = least_squares(minimization, np.zeros(16))['x']\n",
    "        S_hat_n = np.transpose(np.reshape(x_hat_n, (4,4)))\n",
    "        sensitivity_set.append(S_hat_n)\n",
    "\n",
    "        biased_CRLB = S_hat_n@CRLB_n@(S_hat_n.transpose())\n",
    "        biased_CRLB_set.append(biased_CRLB)\n",
    "        \n",
    "        c1_CRLB = biased_CRLB[0,0]\n",
    "        c1_CRLB_set.append(c1_CRLB)\n",
    "        c2_CRLB = biased_CRLB[1,1]\n",
    "        c2_CRLB_set.append(c2_CRLB)\n",
    "        T21_CRLB = biased_CRLB[2,2]\n",
    "        T21_CRLB_set.append(T21_CRLB)\n",
    "        T22_CRLB = biased_CRLB[3,3] \n",
    "        T22_CRLB_set.append(T22_CRLB)\n",
    "        if verbose:\n",
    "            print('Biased CRLB at %s noise realizations:'%(n_set[i]), c1_CRLB, c2_CRLB, T21_CRLB, T22_CRLB)\n",
    "            \n",
    "    return biased_CRLB_set, c1_CRLB_set, c2_CRLB_set, T21_CRLB_set, T22_CRLB_set, sensitivity_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3423482f-f656-4d1b-abcd-431a0528a014",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "theta_n = np.array((0.3, 0.7, 50, 90))\n",
    "c_range = [-0.05, 0.05]\n",
    "T2_range = [-2.5, 2.5]\n",
    "M = 10\n",
    "n_noise_realizations = 100\n",
    "\n",
    "CRLB_n = cov_matrix(theta_n[0],theta_n[1],theta_n[2],theta_n[3], SNR)\n",
    "biased_CRLB_n, sensitivity = biased_CRLB(theta_n, c_range, T2_range, M, n_noise_realizations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b418f8ee-92b5-4052-92ad-194c6051832e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 305/1000 [6:13:29<14:11:03, 73.47s/it] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [49], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m M \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\n\u001b[0;32m      6\u001b[0m n_set \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mlinspace(\u001b[39m100\u001b[39m, \u001b[39m100000\u001b[39m, \u001b[39m1000\u001b[39m, dtype\u001b[39m=\u001b[39m\u001b[39mint\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m biased_CRLB_set, c1_CRLB_set, c2_CRLB_set, T21_CRLB_set, T22_CRLB_set, sensitivity_set \u001b[39m=\u001b[39m check_CRLB_convergence(\n\u001b[0;32m      8\u001b[0m     n_set, theta_n, c_range, T2_range, M, verbose\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "Cell \u001b[1;32mIn [47], line 106\u001b[0m, in \u001b[0;36mcheck_CRLB_convergence\u001b[1;34m(n_set, theta_n, c_range, T2_range, M, verbose)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    105\u001b[0m     n_noise_realizations \u001b[39m=\u001b[39m n_set[i] \u001b[39m-\u001b[39m n_set[i\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m--> 106\u001b[0m     new_estimates \u001b[39m=\u001b[39m run_NR_P_theta_n(P_theta_n, n_noise_realizations)\n\u001b[0;32m    107\u001b[0m     estimates \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mappend(estimates, new_estimates,axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m    108\u001b[0m sample_mean \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean(estimates, axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "Cell \u001b[1;32mIn [47], line 56\u001b[0m, in \u001b[0;36mrun_NR_P_theta_n\u001b[1;34m(P_theta_n, n_noise_realizations)\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_noise_realizations):\n\u001b[0;32m     55\u001b[0m         noise_realization \u001b[39m=\u001b[39m add_noise(noiseless_curve, SNR)\n\u001b[1;32m---> 56\u001b[0m         est \u001b[39m=\u001b[39m estimate_parameters(noise_realization, \u001b[39m0\u001b[39;49m)\n\u001b[0;32m     57\u001b[0m         estimates[:,m,i] \u001b[39m=\u001b[39m est\n\u001b[0;32m     58\u001b[0m \u001b[39mreturn\u001b[39;00m estimates\n",
      "Cell \u001b[1;32mIn [30], line 40\u001b[0m, in \u001b[0;36mestimate_parameters\u001b[1;34m(data, lam, n_initials, post_normalize)\u001b[0m\n\u001b[0;32m     36\u001b[0m p0 \u001b[39m=\u001b[39m [ic1,ic2,iT21,iT22]\n\u001b[0;32m     38\u001b[0m data_tilde \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mappend(data, [\u001b[39m0\u001b[39m,\u001b[39m0\u001b[39m,\u001b[39m0\u001b[39m,\u001b[39m0\u001b[39m])  \n\u001b[1;32m---> 40\u001b[0m (c1e, c2e, T21e, T22e), cov \u001b[39m=\u001b[39m curve_fit(G_tilde(lam), tdata, data_tilde, bounds \u001b[39m=\u001b[39;49m (\u001b[39m0\u001b[39;49m, upper_bound), p0\u001b[39m=\u001b[39;49mp0, max_nfev \u001b[39m=\u001b[39;49m \u001b[39m4000\u001b[39;49m)\n\u001b[0;32m     42\u001b[0m \u001b[39m#Require T22>T21\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[39mif\u001b[39;00m T22e \u001b[39m>\u001b[39m T21e:\n",
      "File \u001b[1;32mc:\\co\\NIA\\.grif_venv\\lib\\site-packages\\scipy\\optimize\\_minpack_py.py:845\u001b[0m, in \u001b[0;36mcurve_fit\u001b[1;34m(f, xdata, ydata, p0, sigma, absolute_sigma, check_finite, bounds, method, jac, full_output, **kwargs)\u001b[0m\n\u001b[0;32m    842\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mmax_nfev\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m kwargs:\n\u001b[0;32m    843\u001b[0m     kwargs[\u001b[39m'\u001b[39m\u001b[39mmax_nfev\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m'\u001b[39m\u001b[39mmaxfev\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m--> 845\u001b[0m res \u001b[39m=\u001b[39m least_squares(func, p0, jac\u001b[39m=\u001b[39mjac, bounds\u001b[39m=\u001b[39mbounds, method\u001b[39m=\u001b[39mmethod,\n\u001b[0;32m    846\u001b[0m                     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    848\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m res\u001b[39m.\u001b[39msuccess:\n\u001b[0;32m    849\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mOptimal parameters not found: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m res\u001b[39m.\u001b[39mmessage)\n",
      "File \u001b[1;32mc:\\co\\NIA\\.grif_venv\\lib\\site-packages\\scipy\\optimize\\_lsq\\least_squares.py:928\u001b[0m, in \u001b[0;36mleast_squares\u001b[1;34m(fun, x0, jac, bounds, method, ftol, xtol, gtol, x_scale, loss, f_scale, diff_step, tr_solver, tr_options, jac_sparsity, max_nfev, verbose, args, kwargs)\u001b[0m\n\u001b[0;32m    924\u001b[0m     result \u001b[39m=\u001b[39m call_minpack(fun_wrapped, x0, jac_wrapped, ftol, xtol, gtol,\n\u001b[0;32m    925\u001b[0m                           max_nfev, x_scale, diff_step)\n\u001b[0;32m    927\u001b[0m \u001b[39melif\u001b[39;00m method \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtrf\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m--> 928\u001b[0m     result \u001b[39m=\u001b[39m trf(fun_wrapped, jac_wrapped, x0, f0, J0, lb, ub, ftol, xtol,\n\u001b[0;32m    929\u001b[0m                  gtol, max_nfev, x_scale, loss_function, tr_solver,\n\u001b[0;32m    930\u001b[0m                  tr_options\u001b[39m.\u001b[39;49mcopy(), verbose)\n\u001b[0;32m    932\u001b[0m \u001b[39melif\u001b[39;00m method \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mdogbox\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    933\u001b[0m     \u001b[39mif\u001b[39;00m tr_solver \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mlsmr\u001b[39m\u001b[39m'\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mregularize\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m tr_options:\n",
      "File \u001b[1;32mc:\\co\\NIA\\.grif_venv\\lib\\site-packages\\scipy\\optimize\\_lsq\\trf.py:123\u001b[0m, in \u001b[0;36mtrf\u001b[1;34m(fun, jac, x0, f0, J0, lb, ub, ftol, xtol, gtol, max_nfev, x_scale, loss_function, tr_solver, tr_options, verbose)\u001b[0m\n\u001b[0;32m    119\u001b[0m     \u001b[39mreturn\u001b[39;00m trf_no_bounds(\n\u001b[0;32m    120\u001b[0m         fun, jac, x0, f0, J0, ftol, xtol, gtol, max_nfev, x_scale,\n\u001b[0;32m    121\u001b[0m         loss_function, tr_solver, tr_options, verbose)\n\u001b[0;32m    122\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 123\u001b[0m     \u001b[39mreturn\u001b[39;00m trf_bounds(\n\u001b[0;32m    124\u001b[0m         fun, jac, x0, f0, J0, lb, ub, ftol, xtol, gtol, max_nfev, x_scale,\n\u001b[0;32m    125\u001b[0m         loss_function, tr_solver, tr_options, verbose)\n",
      "File \u001b[1;32mc:\\co\\NIA\\.grif_venv\\lib\\site-packages\\scipy\\optimize\\_lsq\\trf.py:381\u001b[0m, in \u001b[0;36mtrf_bounds\u001b[1;34m(fun, jac, x0, f0, J0, lb, ub, ftol, xtol, gtol, max_nfev, x_scale, loss_function, tr_solver, tr_options, verbose)\u001b[0m\n\u001b[0;32m    378\u001b[0m     rho \u001b[39m=\u001b[39m loss_function(f)\n\u001b[0;32m    379\u001b[0m     J, f \u001b[39m=\u001b[39m scale_for_robust_loss_function(J, f, rho)\n\u001b[1;32m--> 381\u001b[0m g \u001b[39m=\u001b[39m compute_grad(J, f)\n\u001b[0;32m    383\u001b[0m \u001b[39mif\u001b[39;00m jac_scale:\n\u001b[0;32m    384\u001b[0m     scale, scale_inv \u001b[39m=\u001b[39m compute_jac_scale(J, scale_inv)\n",
      "File \u001b[1;32mc:\\co\\NIA\\.grif_venv\\lib\\site-packages\\scipy\\optimize\\_lsq\\common.py:598\u001b[0m, in \u001b[0;36mcompute_grad\u001b[1;34m(J, f)\u001b[0m\n\u001b[0;32m    596\u001b[0m     \u001b[39mreturn\u001b[39;00m J\u001b[39m.\u001b[39mrmatvec(f)\n\u001b[0;32m    597\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 598\u001b[0m     \u001b[39mreturn\u001b[39;00m J\u001b[39m.\u001b[39;49mT\u001b[39m.\u001b[39;49mdot(f)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Check for convergence\n",
    "theta_n = np.array((0.5, 0.5, 20, 200))\n",
    "c_range = [-0.05, 0.05]\n",
    "T2_range = [-2.5, 2.5]\n",
    "M = 10\n",
    "n_set = np.linspace(100, 100000, 1000, dtype=int)\n",
    "biased_CRLB_set, c1_CRLB_set, c2_CRLB_set, T21_CRLB_set, T22_CRLB_set, sensitivity_set = check_CRLB_convergence(\n",
    "    n_set, theta_n, c_range, T2_range, M, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24a8e93-d5f5-4fd5-95e7-8e51ea9c70ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "CRLB_n = cov_matrix(theta_n[0],theta_n[1],theta_n[2],theta_n[3], SNR)\n",
    "c1_CRLB = CRLB_n[0,0]\n",
    "c2_CRLB = CRLB_n[1,1]\n",
    "T21_CRLB = CRLB_n[2,2]\n",
    "T22_CRLB = CRLB_n[3,3]\n",
    "fig, ax = plt.subplots(1,4,figsize=(18,4),tight_layout=True)\n",
    "fig.suptitle('Convergence of biased CRLB at SNR %s'%SNR)\n",
    "ax[0].plot(n_set, c1_CRLB_set, label='c1 biased CRLB')\n",
    "ax[0].axhline(y=c1_CRLB, label = 'c1 CRLB', color='r')\n",
    "ax[0].set_xlabel('Noise realizations')\n",
    "ax[0].set_ylabel('CRLB')\n",
    "ax[0].set_title('c1 convergence')\n",
    "ax[0].legend()\n",
    "ax[1].plot(n_set, c2_CRLB_set, label='c2 biased CRLB')\n",
    "ax[1].axhline(y=c2_CRLB, label = 'c2 CRLB', color='r')\n",
    "ax[1].set_xlabel('Noise realizations')\n",
    "ax[1].set_ylabel('CRLB')\n",
    "ax[1].set_title('c2 convergence')\n",
    "ax[1].legend()\n",
    "ax[2].plot(n_set, T21_CRLB_set, label='T21 biased CRLB')\n",
    "ax[2].axhline(y=T21_CRLB, label = 'T21 CRLB', color='r')\n",
    "ax[2].set_xlabel('Noise realizations')\n",
    "ax[2].set_ylabel('CRLB')\n",
    "ax[2].set_title('T21 convergence')\n",
    "ax[2].legend()\n",
    "ax[3].plot(n_set, T22_CRLB_set, label='T22 biased CRLB')\n",
    "ax[3].axhline(y=T22_CRLB, label = 'T22 CRLB', color='r')\n",
    "ax[3].set_xlabel('Noise realizations')\n",
    "ax[3].set_ylabel('CRLB')\n",
    "ax[3].set_title('T22 convergence')\n",
    "ax[3].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b923597c-9e2a-4059-be1a-2b1ad03e124c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with h5py.File('Brain MWF Maps//Biased CRLB SNR 1 Convergence, 1e5 NR 5-22.hdf5','a') as f2:\n",
    "#     f2.create_dataset('biased CRLB', data=biased_CRLB_set)\n",
    "#     f2.create_dataset('sensitivity', data=sensitivity_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206f1aa6",
   "metadata": {},
   "source": [
    "# Function Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e9636d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "c1_trial = [0.4]\n",
    "T21_trial = [40]\n",
    "T22_trial = [100,200]\n",
    "product_expected = [[0.4,0.6,40,100],[0.4,0.6,40,200]]\n",
    "\n",
    "product_result = construct_paramList(c1_trial, T21_trial, T22_trial)\n",
    "\n",
    "assert(np.all(product_expected == product_result))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7 (tags/v3.10.7:6cc6b13, Sep  5 2022, 14:08:36) [MSC v.1933 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "1f9461e57c8aaeccb5a4202f3eb0df437b94cded3914cf93be397abab25dc6de"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

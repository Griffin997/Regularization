{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Brain Curves Analysis\n",
    "\n",
    "This code takes the functions and data used in the main_brainReg_genBrainData.py file to then assess a small sample of those pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# Libaries ###############\n",
    "\n",
    "import h5py\n",
    "import scipy\n",
    "import scipy.io\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.optimize import least_squares\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statistics\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import time\n",
    "import itertools\n",
    "from itertools import product, zip_longest\n",
    "import pickle\n",
    "from tqdm import tqdm, trange\n",
    "from datetime import date\n",
    "\n",
    "import multiprocess as mp\n",
    "from multiprocessing import Pool, freeze_support\n",
    "from multiprocessing import set_start_method\n",
    "\n",
    "import sys\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "parent = os.path.dirname(os.path.abspath(''))\n",
    "sys.path.append(parent)\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# Data Set Options & Hyperparameters ############\n",
    "\n",
    "add_noise = True          #Add noise to the data beyond what is there naturally\n",
    "add_mask = True             #Add a mask to the data - this mask eliminates data below a threshold (mas_amplitude)\n",
    "apply_normalizer = True     #Normalizes the data during the processing step\n",
    "estimate_offset = True      #Adds an offset to the signal that is estimated\n",
    "subsection = True           #Looks at a region a sixteenth of the full size\n",
    "multistart_method = False    #Applies a multistart method for each parameter fitting instance\n",
    "MB_model = False           #This model incoroporates the normalization and offset to a three parameter fit\n",
    "\n",
    "# The MB_model does the normalization as part of the algorithm\n",
    "if MB_model: assert(not apply_normalizer and not estimate_offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing 4 pixels\n"
     ]
    }
   ],
   "source": [
    "############## Frequently Changed Parameters ###########\n",
    "\n",
    "n_lambdas = 101\n",
    "\n",
    "SNR_goal = 100\n",
    "\n",
    "if add_noise:\n",
    "    iterations = 15\n",
    "else:\n",
    "    iterations = 1\n",
    "\n",
    "#Order of pixels (left of vc, above vc, white matter towards center, white matter towards periphery)\n",
    "pixels = np.array([[52,38],[40,62],[40,37],[42,18]])\n",
    "print(f'Analyzing {pixels.shape[0]} pixels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## Initializing Data ##########\n",
    "\n",
    "brain_data = scipy.io.loadmat(os.getcwd() + '/MB_References/BLSA_1742_04_MCIAD_m41/NESMA_slice5.mat')\n",
    "I_raw = brain_data['slice_oi']\n",
    "\n",
    "if subsection:\n",
    "    I_raw_x = I_raw.shape[0]//4\n",
    "    I_raw_y = I_raw.shape[1]//4\n",
    "    I_raw = I_raw[I_raw_x:2*I_raw_x, I_raw_y:2*I_raw_y, :]\n",
    "\n",
    "n_vert, n_hori, n_elements_brain = I_raw.shape\n",
    "\n",
    "t_increment_brain = 11.3 #This is a measurement originally indicated by Chuan Bi in the initial email about this data\n",
    "tdata = np.linspace(t_increment_brain, (n_elements_brain)*(t_increment_brain), n_elements_brain)\n",
    "\n",
    "# #This is how we will keep track of all voxels that are called --------------------------------- no iterator for voxels needed\n",
    "# target_iterator = np.array([item for item in itertools.product(np.arange(0,n_vert,1), np.arange(0,n_hori,1))])\n",
    "\n",
    "# all pixels with a lower mask amplitude are considered to be free water (i.e. vesicles)\n",
    "mask_amplitude = 700\n",
    "\n",
    "############# Global Params ###############\n",
    "\n",
    "#These bounds were chosen to match the simulated data while also being restrictive enough\n",
    "#This provides a little extra space as the hard bounds would be [1,1,50,300]\n",
    "if MB_model:\n",
    "    upper_bound = [np.inf, 0.5, 60, 2000] #c1 used to be 0.5 - changed to 1 for complete range\n",
    "elif not apply_normalizer:\n",
    "    upper_bound = [0.5,1.2,60,2000]\n",
    "else:\n",
    "    upper_bound = [0.5,1.2,60,2000]\n",
    "\n",
    "if estimate_offset or MB_model:\n",
    "    upper_bound.append(np.inf)\n",
    "\n",
    "lambdas = np.append(0, np.logspace(-5,1, n_lambdas))\n",
    "\n",
    "ob_weight = 1\n",
    "if MB_model:\n",
    "    agg_weights = np.array([1/ob_weight, 1, 1/ob_weight, 1/ob_weight])\n",
    "if not apply_normalizer:\n",
    "    agg_weights = np.array([1,1,1,1])\n",
    "else:\n",
    "    agg_weights = np.array([1, 1, 1/ob_weight, 1/ob_weight])\n",
    "\n",
    "if multistart_method:\n",
    "    num_nultistarts = 10\n",
    "    ms_upper_bound = [1,60,300] \n",
    "else:\n",
    "    num_multistarts = 1\n",
    "    ms_upper_bound = [0] \n",
    "\n",
    "SNR_collect = np.zeros(iterations)\n",
    "\n",
    "if subsection:\n",
    "    vert1 = 37\n",
    "    vert2 = 47\n",
    "    hori1 = 25\n",
    "    hori2 = 70\n",
    "else:\n",
    "    vert1 = 165             #60     #108\n",
    "    vert2 = 180            #125     #116\n",
    "    hori1 = 120            #100      #86\n",
    "    hori2 = 180            #115      #93\n",
    "\n",
    "vBox = (vert1,vert1,vert2,vert2,vert1)\n",
    "hBox = (hori1,hori2,hori2,hori1,hori1)\n",
    "\n",
    "noiseRegion = [vert1,vert2,hori1,hori2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# Signal Functions ##############\n",
    "\n",
    "def G(t, con_1, con_2, tau_1, tau_2): \n",
    "    function = con_1*np.exp(-t/tau_1) + con_2*np.exp(-t/tau_2)\n",
    "    return function\n",
    "\n",
    "def G_off(t, con_1, con_2, tau_1, tau_2, offSet): \n",
    "    function = con_1*np.exp(-t/tau_1) + con_2*np.exp(-t/tau_2) + offSet\n",
    "    return function\n",
    "\n",
    "def G_MB(t, alpha, beta, tau_1, tau_2, offSet):\n",
    "    function = alpha*(beta*np.exp(-t/tau_1) + (1-beta)*np.exp(-t/tau_2)) + offSet\n",
    "    return function\n",
    "\n",
    "def G_tilde(lam, SA = 1, offSet = estimate_offset, opt_MB = MB_model):\n",
    "    #SA defines the signal amplitude, defaults to 1 for simulated data\n",
    "    if offSet:\n",
    "        def Gt_lam(t, con1, con2, tau1, tau2, oS):\n",
    "            return np.append(G_off(t, con1, con2, tau1, tau2, oS), [lam*con1/SA, lam*con2/SA, lam*tau1/ob_weight, lam*tau2/ob_weight])\n",
    "    elif opt_MB:\n",
    "        def Gt_lam(t, alpha, beta, tau1, tau2, oS):\n",
    "            return np.append(G_MB(t, alpha, beta, tau1, tau2, oS), [lam*alpha/ob_weight, lam*beta/SA, lam*tau1/ob_weight, lam*tau2/ob_weight])\n",
    "    else:\n",
    "        def Gt_lam(t, con1, con2, tau1, tau2):\n",
    "            return np.append(G(t, con1, con2, tau1, tau2), [lam*con1/SA, lam*con2/SA, lam*tau1/ob_weight, lam*tau2/ob_weight])\n",
    "    return Gt_lam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# Data Processing Functions ##############\n",
    "\n",
    "def mask_data(raw, mask_amplitude):\n",
    "    #Sets every decay curve in the data set where the amplitude is less than a threshold value to zero\n",
    "    I_masked = np.copy(raw)\n",
    "    I_masked[I_masked[:,:,0]<mask_amplitude] = 0\n",
    "    return I_masked\n",
    "\n",
    "def calculate_brain_SNR(raw, region):\n",
    "    #calculates the SNR of the brain using a homogenous region fed into the \n",
    "    v1,v2,h1,h2 = region\n",
    "\n",
    "    rawZone = raw[v1:v2,h1:h2,:]\n",
    "\n",
    "    regionZero = rawZone[:, :, 0]\n",
    "    regionZero_mean = np.mean(regionZero)\n",
    "\n",
    "    regionEnd = rawZone[:, :, -3:] #last three points across the entire sampled region\n",
    "    regionEnd_std = np.std(regionEnd)\n",
    "    regionEnd_absMean = np.mean(np.abs(regionEnd))\n",
    "\n",
    "    SNR_region = (regionZero_mean - regionEnd_absMean)/regionEnd_std\n",
    "\n",
    "    return SNR_region\n",
    "\n",
    "def normalize_brain(I_data):\n",
    "    n_vert, n_hori, n_elem = I_data.shape\n",
    "    I_normalized = np.zeros(I_data.shape)\n",
    "    for i_vert in range(n_hori):\n",
    "        for i_hori in range(n_vert):\n",
    "            data = I_data[i_vert,i_hori,:]\n",
    "            if data[0]>0:\n",
    "                data_normalized = data/(data[0])\n",
    "            else:\n",
    "                data_normalized = np.zeros(n_elements_brain)\n",
    "            I_normalized[i_vert,i_hori,:] = data_normalized\n",
    "    return I_normalized\n",
    "\n",
    "def add_noise_brain_uniform(raw, SNR_desired, region, I_mask_factor):\n",
    "    #This function was built with the intention of taking a region (e.g. the homogenous region to the right of the ventricles)\n",
    "    #Add noise to make sure the final SNR is close to the desired SNR\n",
    "\n",
    "    v1,v2,h1,h2 = region\n",
    "\n",
    "    rawZone = raw[v1:v2,h1:h2,:]\n",
    "\n",
    "    regionZero = rawZone[:, :, 0]\n",
    "    sigRef = np.mean(regionZero)\n",
    "\n",
    "    regionEnd = rawZone[:, :, -3:]\n",
    "    initSD = np.std(regionEnd)\n",
    "\n",
    "    addSD = (sigRef**2/SNR_desired**2 - initSD**2)**(1/2)\n",
    "\n",
    "    noiseMat = np.random.normal(0,addSD,raw.shape)\n",
    "    I_noised = raw + noiseMat*I_mask_factor\n",
    "\n",
    "    return I_noised, addSD\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## Parameter Estimation Functions ###############\n",
    "\n",
    "def generate_p0(ms_ub = ms_upper_bound, offSet = estimate_offset, ms_opt = multistart_method, sig_init = 1):\n",
    "    \n",
    "    \n",
    "    if MB_model:\n",
    "        init_params = (sig_init, 0.2, 20, 80, 1)\n",
    "    elif not ms_opt:\n",
    "        init_params = (sig_init*0.2, sig_init*0.8, 20, 80, 1)\n",
    "    else:\n",
    "        three_params = np.random.uniform(0,1,3)*ms_ub\n",
    "        init_params = (sig_init*three_params[0], sig_init*(1-three_params[0]), three_params[1], three_params[2], 1)\n",
    "\n",
    "    if not offSet and not MB_model:\n",
    "        init_params = init_params[0:4] #cleave the 5th parameter\n",
    "\n",
    "    return init_params\n",
    "\n",
    "def check_param_order(popt):\n",
    "    #Function to automate the order of parameters if desired\n",
    "    #Reshaping of array to ensure that the parameter pairs all end up in the appropriate place - ensures that T22 > T21\n",
    "    if (popt[2] > popt[3]): #We want by convention to make sure that T21 is <= T22\n",
    "        p_hold = popt[0]\n",
    "        popt[0] = popt[1]\n",
    "        popt[1] = p_hold\n",
    "\n",
    "        p_hold = popt[2]\n",
    "        popt[2] = popt[3]\n",
    "        popt[3] = p_hold\n",
    "    return popt\n",
    "\n",
    "def estimate_parameters(data, lam, n_initials = num_multistarts):\n",
    "    #Pick n_initials random initial conditions within the bound, and choose the one giving the lowest model-data mismatch residual\n",
    "\n",
    "    parameter_tail = [0,0,0,0]\n",
    "    data_tilde = np.append(data, parameter_tail) # Adds zeros to the end of the regularization array for the param estimation\n",
    "    \n",
    "    RSS_hold = np.inf\n",
    "    obj_hold = np.inf\n",
    "    for i in range(n_initials):\n",
    "\n",
    "        np.random.seed(i) #Only has an effect on the multistart strategy\n",
    "        if MB_model or not apply_normalizer:\n",
    "            init_params = generate_p0(sig_init = data_tilde[0])\n",
    "        else:\n",
    "            init_params = generate_p0()\n",
    "\n",
    "        temp_upper_bound = np.copy(upper_bound)\n",
    "        if not apply_normalizer and not MB_model:\n",
    "            temp_upper_bound[0] = data_tilde[0]*upper_bound[0]\n",
    "            temp_upper_bound[1] = data_tilde[0]*upper_bound[1]\n",
    "        \n",
    "        try:\n",
    "            if estimate_offset or MB_model:\n",
    "                lower_bound = [0,0,0,0,0]\n",
    "            else:\n",
    "                lower_bound = [0,0,0,0]\n",
    "            popt, _ = curve_fit(G_tilde(lam), tdata, data_tilde, bounds = (lower_bound, temp_upper_bound), p0=init_params, max_nfev = 4000)\n",
    "        except Exception as error:\n",
    "            if estimate_offset or MB_model:\n",
    "                popt = [0,0,1,1,0]\n",
    "            else:\n",
    "                popt = [0,0,1,1]\n",
    "            print(\"Error in parameter fitting: \" + str(error))\n",
    "\n",
    "        if estimate_offset:\n",
    "            est_curve = G_off(tdata,*popt)\n",
    "        elif MB_model:\n",
    "            est_curve = G_MB(tdata,*popt)\n",
    "        else:\n",
    "            est_curve = G(tdata,*popt)\n",
    "        RSS_temp = np.sum((est_curve - data)**2)\n",
    "\n",
    "        obj_pTemp = lam*agg_weights*popt[0:4]\n",
    "        obj_temp = RSS_temp + np.linalg.norm(obj_pTemp)\n",
    "\n",
    "        if obj_temp < obj_hold:\n",
    "            obj_hold = obj_temp\n",
    "            best_popt = popt\n",
    "            RSS_hold = RSS_temp\n",
    "        \n",
    "    if not MB_model:\n",
    "        popt = check_param_order(best_popt)\n",
    "    else:\n",
    "        popt = best_popt\n",
    " \n",
    "    return popt, RSS_hold\n",
    "\n",
    "def generate_all_estimates(i_vert, i_hori, brain_data_3D):\n",
    "    #Generates a comprehensive matrix of all parameter estimates for all param combinations, \n",
    "    #noise realizations, SNR values, and lambdas of interest\n",
    "\n",
    "    #This function had to be adjusted to account for individual voxels fed in rather than a list of voxels\n",
    "\n",
    "    # i_vert, i_hori = target_iterator[i_voxel]--------------------- not used\n",
    "    noise_data = brain_data_3D[i_vert, i_hori, :]\n",
    "    e_lis = []\n",
    "\n",
    "    for iLam in range(len(lambdas)):    #Loop through all lambda values\n",
    "        e_df = pd.DataFrame(columns = [\"Data\", \"Indices\", \"Estimates\", \"RSS\"])\n",
    "        lam = lambdas[iLam]\n",
    "\n",
    "        if np.all(noise_data == 0):\n",
    "            if estimate_offset or MB_model:\n",
    "                param_estimates = [0,0,1,1,0]\n",
    "            else:\n",
    "                param_estimates = [0,0,1,1]\n",
    "            RSS_estimate = np.inf\n",
    "        else:\n",
    "            param_estimates, RSS_estimate = estimate_parameters(noise_data, lam)\n",
    "        \n",
    "        assert(noise_data.shape[0] == n_elements_brain)\n",
    "        e_df[\"Data\"] = [noise_data]\n",
    "        e_df[\"Indices\"] = [[i_vert, i_hori]]\n",
    "        e_df[\"Estimates\"] = [param_estimates]\n",
    "        e_df[\"RSS\"] = [RSS_estimate]\n",
    "        e_lis.append(e_df)\n",
    "    \n",
    "    return pd.concat(e_lis, ignore_index= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### This ensures that the same mask is applied throughout\n",
    "\n",
    "if add_mask:\n",
    "    I_masked = mask_data(I_raw, mask_amplitude)\n",
    "    I_mask_factor = (I_masked!=0)\n",
    "else:\n",
    "    I_masked = I_raw\n",
    "    I_mask_factor = (I_masked!=0)\n",
    "\n",
    "#### Looping through Iterations of the brain - applying parallel processing to improve the speed\n",
    "\n",
    "for iter in trange(iterations):    #Build {iterations} number of noisey brain realizations for each of the test cases\n",
    "\n",
    "    np.random.seed(iter)\n",
    "\n",
    "    if add_noise:\n",
    "        I_noised = add_noise_brain_uniform(I_masked, SNR_goal, noiseRegion, I_mask_factor)[0]\n",
    "    else:\n",
    "        I_noised = I_masked\n",
    "\n",
    "    if apply_normalizer:\n",
    "        noise_iteration = normalize_brain(I_noised)\n",
    "    else:\n",
    "        noise_iteration = I_noised\n",
    "\n",
    "    SNR_collect[iter] = calculate_brain_SNR(noise_iteration, noiseRegion)\n",
    "\n",
    "    for pix in range(pixels.shape[0]):\n",
    "        lis = []\n",
    "\n",
    "            with tqdm(total=target_iterator.shape[0]) as pbar:\n",
    "                for estimates_dataframe in pool.imap_unordered(lambda hold_label: generate_all_estimates(hold_label, noise_iteration), range(target_iterator.shape[0])):\n",
    "\n",
    "                    lis.append(estimates_dataframe)\n",
    "\n",
    "                    pbar.update()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
